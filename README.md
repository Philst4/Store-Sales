ğŸ§± Project Structure
project/
â”‚
â”œâ”€â”€ data/                # Raw & processed datasets (ignored via .gitignore)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ process_data.py  # Cleans + engineers features
â”‚   â”œâ”€â”€ tune_model.py    # Runs Optuna study
â”‚   â”œâ”€â”€ train_best.py    # Trains final model using best trial
â”‚   â””â”€â”€ make_submission.py
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ pipeline_local.ipynb
â”‚   â””â”€â”€ pipeline_colab.ipynb
â”‚
â”œâ”€â”€ environment.yml
â””â”€â”€ README.md
ğŸ› ï¸ Setup
1. ğŸ“¥ Running on Google Colab (Easiest)
Open pipeline_colab.ipynb in Colab
Run all cells â€” the notebook handles:
Installing dependencies
Downloading dataset
Running the full pipeline
No local setup required.
2. ğŸ–¥ï¸ Running Locally (Terminal Workflow)
Step 1 â€” Clone the repository
git clone https://github.com/Philst4/Store-Sales.git
cd Store-Sales
Step 2 â€” Create and activate the conda environment
conda env create -f environment.yml
conda activate store-sales
Step 3 â€” Run the full pipeline
# (1) Data processing & feature engineering
python scripts/process_data.py

# (2) Hyperparameter tuning with Optuna
python scripts/tune_model.py

# (3) Train best model found by tuning
python scripts/train_best.py

# (4) Generate submission.csv
python scripts/make_submission.py
This produces:
data/processed/
models/
submissions/submission.csv
optuna_studies.db
3. ğŸ§ª Running Locally via Notebook
If you prefer a notebook workflow:
Open pipeline_local.ipynb
Ensure your conda environment is active
Run the notebook top-to-bottom
(it mirrors the CLI pipeline)
ğŸ“Š Technologies Used
Python
XGBoost / LightGBM
Optuna (hyperparameter optimization)
Pandas / NumPy
Matplotlib / Seaborn
Conda (reproducibility)
Jupyter / Google Colab
ğŸ¯ Skills Demonstrated
This project showcases abilities valued in Data Science, Machine Learning Engineering, and Software Engineering roles:
ğŸ§  Machine Learning
Advanced feature engineering
Model tuning with Bayesian optimization
Cross-validation and evaluation strategies
ğŸ—ï¸ Software Engineering
Modular, maintainable Python scripts
Reproducible environments
Clear project structure
Automated pipelines
ğŸ“¦ End-to-End Deployment Thinking
Full reproducibility (local + cloud)
Clear execution pathways for technical interview reviewers
Script-driven workflow that reflects production pipelines
ğŸ“ Notes
The dataset comes from the Kaggle Store Sales Forecasting competition.
Data folders are excluded via .gitignore; users must download them automatically via script/notebook.
